{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembled Transferred Enbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensembled Transferred Embeddings** \n",
    "\n",
    "- Installation and imports\n",
    "\n",
    "- Read and preprocess data\n",
    "\n",
    "- Uninformative detection\n",
    "    - Train Autoencoder\n",
    "    - Extract transferred embeddings from Autoencoder\n",
    "    - Train Logistic regression model with transferred embeddings and labels from Mturk\n",
    "    - Evaluate with ROC curve\n",
    "\n",
    "- Item categorization\n",
    "    - Preprocess text for the network\n",
    "    - Train goods/services classifier\n",
    "    - Train model on invoices with goods categories and model on invoices with service categories\n",
    "    - Train model on eBay data\n",
    "    - Extract transferred embeddings from each model\n",
    "    - Build models with each embedding with Mturk labels (invoice-service embedding with service categories and \n",
    "                                                          invoice-goods and eBay embeddings with goods categories)\n",
    "    - Predict on the test set\n",
    "    - Evaluate using accuracy and F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation in imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "colab_type": "code",
    "id": "66q680NJyLqP",
    "outputId": "c380e68f-7528-4568-873e-766450adf48b"
   },
   "outputs": [],
   "source": [
    "!pip install -q keras\n",
    "!pip install -q gensim\n",
    "!pip install scikit-plot\n",
    "!pip install --upgrade tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "BPM2nviCyP0Y",
    "outputId": "cddfc664-a97f-472e-f9ec-9ab3a62e1ab2"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XFzWhaN7yaHg",
    "outputId": "e23b0f7b-b72b-495a-943c-d4efa308e263"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GlobalAveragePooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import nltk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import gensim \n",
    "\n",
    "\n",
    "\n",
    "def pre_processing(text):\n",
    "#     ps = PorterStemmer()\n",
    "#     maping = {'tshirt':'t-shirt','airpos':'headphones','vguc':'very good used condition','nwot':'new with tag','jeggings':'leggings','qty':'quantity',\n",
    "# 'lipsense':'lipstick','fbid':'facebook id','xlarge':'xl','lampwork':'glasswork','druzy':'druse','vneck':'v-neck'}\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokenizer = RegexpTokenizer()\n",
    "    letters_only = re.sub(r'[\\W_]+', \" \", text)  # Removing punctuation + numbers\n",
    "#     letters_only = clean_numbers(letters_only)\n",
    "    tokens = letters_only.lower().split(' ')  # Tokenizing\n",
    "#     clean_words = [word if word not in maping else maping[word] for word in tokens if word not in stop_words]  # Stop words\n",
    "#     stem_words = [product_id(w) for w in clean_words]  # Stemming\n",
    "#     stem_words = [ps.stem(w) for w in clean_words]  # Stemming\n",
    "    sentences = \" \".join(tokens)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_data_path = '/content/drive/My Drive/Colab Notebooks/paypal/paypal.csv'\n",
    "ebay_data_path = '/content/drive/My Drive/Colab Notebooks/paypal/ebay.csv'\n",
    "invoice_test_path = '/content/drive/My Drive/Colab Notebooks/paypal/test_set_9_3.csv'\n",
    "unknowns_path = '/content/drive/My Drive/Colab Notebooks/paypal/unsolvable items.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "yTmLj7Auyw_T",
    "outputId": "9efaffb2-e070-4945-bfe9-1c8a1b7cb409"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "invoice_data = pd.read_csv(invoice_data_path)\n",
    "                        \n",
    "ebay_data = pd.read_csv(ebay_data_path)\n",
    "ebay_data = ebay_data[(pd.notnull(ebay_data['text'])) & (pd.notnull(ebay_data['vertical']))]\n",
    "invoice_test = pd.read_csv(invoice_test_path,encoding = \"ISO-8859-1\")\n",
    "unknowns = pd.read_csv(unknowns_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebay_data = ebay_data.drop_duplicates(['ebay_item_name'])\n",
    "invoice_test = invoice_test.drop_duplicates(['item_name','description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zm5XLn80eNv9"
   },
   "outputs": [],
   "source": [
    "#preprocess text- concatinate item name and description and remove\n",
    "invoice_data['text'] = invoice_data.apply(lambda x: x['item_name'] if str(x['description'])=='nan' else x['item_name'] +' '+ x['description'],axis=1)\n",
    "invoice_data['text'] = invoice_data['text'].apply(lambda x: pre_processing(x))\n",
    "\n",
    "ebay_data['text'] = ebay_data['ebay_item_name'].apply(lambda x: pre_processing(x))\n",
    "\n",
    "invoice_test['text'] = invoice_test.apply(lambda x: x['ITEM NAME'] if str(x['item description'])=='nan' else x['ITEM NAME'] +' '+ x['item description'],axis=1)\n",
    "invoice_test['text'] = invoice_test['text'].apply(lambda x: pre_processing(x))\n",
    "\n",
    "invoice_test = invoice_test[invoice_test.category.notnull()]\n",
    "invoice_test = invoice_test[invoice_test.text.notnull()]\n",
    "\n",
    "unknowns['text'] = unknowns.apply(lambda x: x['ITEM NAME'] if str(x['item description'])=='nan' else x['ITEM NAME'] +' '+ x['item description'],axis=1)\n",
    "unknowns['text'] = unknowns['text'].apply(lambda x: pre_processing(x))\n",
    "unknowns['category'] ='unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiA84CEpy-Wo"
   },
   "outputs": [],
   "source": [
    "#set good and services categories\n",
    "ebay_categories = ['fashion', 'auto-parts', 'cellphones', 'houseware', 'electronics',\n",
    "       'sports-equip', 'memorabilia', 'toys', 'music-videos', 'arts-n-craft',\n",
    "       'jewelry', 'cosmetics', 'computer-hardware', 'garden-equip', 'food-n-drink' ,\n",
    "       'office-supplies', 'books', 'health', 'baby-products', 'pet-supplies',\n",
    "       'furniture', 'nutritional-supp', 'cameras', 'food-n-drink', 'coins',\n",
    "        'music-instruments', 'software','tickets']\n",
    "invoice_data['is_ebay'] = invoice_data.true_indy_name.isin(ebay_categories)\n",
    "invoice_test['is_ebay'] = invoice_test.category.isin(ebay_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9NEx70FzQl6"
   },
   "outputs": [],
   "source": [
    "#split to train and test\n",
    "ind = np.random.random(len(invoice_test)) < 0.75\n",
    "invoice_train_manual = invoice_test[ind]\n",
    "invoice_test = invoice_test[~ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gulX8yF7zUPl",
    "outputId": "fbf5a111-604d-4de6-d090-d2471ac35701"
   },
   "outputs": [],
   "source": [
    "#add unknown to test set\n",
    "ind = np.random.random(len(unknowns))<0.75\n",
    "invoice_test = pd.concat([invoice_test,unknowns[~ind]],axis=0)\n",
    "invoice_train_manual = pd.concat([invoice_train_manual,unknowns[ind]],axis=0)\n",
    "invoice_test['unknown']=invoice_test['category'] =='unknown'\n",
    "invoice_train_manual['unknown']=invoice_train_manual['category'] =='unknown'\n",
    "invoice_test['unknown'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "Ic-NFQbizbSN",
    "outputId": "4c5845c7-0645-463f-c6c3-fba88b4e4d62"
   },
   "outputs": [],
   "source": [
    "#download glove word embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "!wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "t3oHv03wzcEH",
    "outputId": "08c3c0d5-505f-4633-e1b1-165ac0a5137f"
   },
   "outputs": [],
   "source": [
    "#load glove model to memory\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"glove.6B.300d.txt\", word2vec_output_file=\"gensim_glove_vectors.txt\")\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "e9Ki6SgtpUq1",
    "outputId": "8331145d-fe83-4a34-838a-182e45d38732"
   },
   "outputs": [],
   "source": [
    "#build embedding matrix\n",
    "num_words = 30000\n",
    "maxlen=15\n",
    "tokenizer = Tokenizer(num_words = num_words, split=' ')\n",
    "tokenizer.fit_on_texts(invoice_data['text'].values)\n",
    "\n",
    "a = []\n",
    "non=0\n",
    "embed_dim=300\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, embed_dim))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    if word in glove_model.wv:\n",
    "        embedding_vector = glove_model.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    elif i<num_words:\n",
    "      non+=1\n",
    "      a.append(word)\n",
    "embedding_matrix = embedding_matrix[:num_words,:]\n",
    "non"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRcAb4hslE5Z"
   },
   "source": [
    "## Uninformative detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "NtMiFVC3rbVa",
    "outputId": "ea8153de-5bcb-4b6d-a6b9-b4f95a0ce888"
   },
   "outputs": [],
   "source": [
    "#build autoencoder model\n",
    "from  keras.optimizers import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Dropout, RepeatVector\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "#parse all sentences\n",
    "sentenses = invoice_data.text.str.split().values\n",
    "#preprocess for keras\n",
    "num_words=30000\n",
    "maxlen=15\n",
    "tokenizer = Tokenizer(num_words = num_words, split=' ')\n",
    "tokenizer.fit_on_texts(sentenses)\n",
    "seqs = tokenizer.texts_to_sequences(sentenses)\n",
    "pad_seqs = []\n",
    "for i in seqs:\n",
    "    if len(i)>4:\n",
    "        pad_seqs.append(i)\n",
    "pad_seqs = pad_sequences(pad_seqs,maxlen)\n",
    "#The model\n",
    "embed_dim = 150\n",
    "latent_dim = 30\n",
    "batch_size = 502\n",
    "encoder_inputs = Input(shape=(maxlen,), name='Encoder-Input')\n",
    "emb_layer = Embedding(num_words, embed_dim,input_length = maxlen, name='Body-Word-Embedding', mask_zero=False)\n",
    "x = emb_layer(encoder_inputs)\n",
    "state_h = GRU(latent_dim, name='Encoder-Last-GRU')(x)\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "decoded = RepeatVector(maxlen)(seq2seq_encoder_out)\n",
    "decoder_gru = GRU(latent_dim, return_sequences=True, name='Decoder-GRU-before')\n",
    "decoder_gru_output = decoder_gru(decoded)\n",
    "decoder_dense = Dense(num_words, activation='softmax', name='Final-Output-Dense-before')\n",
    "decoder_outputs = decoder_dense(decoder_gru_output)\n",
    "seq2seq_Model = Model(encoder_inputs,decoder_outputs )\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
    "history = seq2seq_Model.fit(pad_seqs, np.expand_dims(pad_seqs, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_split=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "7aXO5263q_f9",
    "outputId": "bdf8794d-859e-466f-a2d4-f37c436dad30"
   },
   "outputs": [],
   "source": [
    "#Feature extraction from autoencoder\n",
    "X_train = tokenizer.texts_to_sequences(invoice_train_manual['text'].values)\n",
    "X_train = pad_sequences(X_train,maxlen=maxlen)\n",
    "X_train = encoder_model.predict(X_train, verbose=1)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(invoice_test['text'].values)\n",
    "X_test = pad_sequences(X_test,maxlen=maxlen)\n",
    "X_test = encoder_model.predict(X_test, verbose=1)\n",
    "#classifier on the autoencoder embedding\n",
    "lr = LogisticRegression().fit(X_train,invoice_train_manual.unknown)\n",
    "lr.score(X_test,invoice_test.unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "dCAdNhan4_OJ",
    "outputId": "13318145-1943-45c1-d908-962998596359"
   },
   "outputs": [],
   "source": [
    "#roc curve\n",
    "from scikitplot.metrics import plot_roc\n",
    "plot_roc(invoice_test.unknown, lr.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwlRAR-KkxQ9"
   },
   "source": [
    "## Item categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-9oM-q3SdHn1"
   },
   "outputs": [],
   "source": [
    "#preprocess text for model\n",
    "tokenizer = Tokenizer(num_words = num_words, split=' ')\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(invoice_data['true_indy_name'].astype(str).values)\n",
    "\n",
    "tokenizer.fit_on_texts(invoice_data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(invoice_data['text'].values)\n",
    "X = pad_sequences(X,maxlen=maxlen)\n",
    "X_test = tokenizer.texts_to_sequences(invoice_test['text'][invoice_test['category'].isin(le.classes_)].values)\n",
    "X_test = pad_sequences(X_test,maxlen=maxlen)\n",
    "Y =le.transform(invoice_data['true_indy_name'].astype(str)).reshape(-1, 1)\n",
    "\n",
    "Y_test = le.transform(invoice_test['category'][invoice_test['category'].isin(le.classes_)]).reshape(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "1FfwPTu_Pm-D",
    "outputId": "1dcbe748-c9ff-423a-e24d-167b52bf3921"
   },
   "outputs": [],
   "source": [
    "#Build goods or service classifier\n",
    "from  keras.optimizers import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Dropout\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, LSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "num_words = 30000\n",
    "maxlen=15\n",
    "\n",
    "embed_dim = 300\n",
    "lstm_out = 200\n",
    "batch_size= 256\n",
    "\n",
    "# ##Buidling the LSTM network\n",
    "\n",
    "\n",
    "inp_phisical = Input(shape=(maxlen, ))\n",
    "\n",
    "x_phisical = Embedding(num_words, embed_dim,input_length = X.shape[1], trainable=False, weights=[embedding_matrix])(inp_phisical)\n",
    "# x = Embedding(num_words, embed_dim,input_length = X.shape[1], trainable=True)(inp)\n",
    "x_phisical = SpatialDropout1D(0.3)(x_phisical)\n",
    "x_phisical = LSTM(lstm_out, return_sequences=True)(x_phisical)\n",
    "x_phisical = LSTM(100)(x_phisical)\n",
    "x_phisical = Dropout(0.2)(x_phisical)\n",
    "dens_phisical = Dense(30)(x_phisical)\n",
    "outp_phisical = Dense(1, activation=\"sigmoid\")(dens_phisical)\n",
    "    \n",
    "model_phisical = Model(inputs=inp_phisical, outputs=outp_phisical)\n",
    "model_phisical.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model_phisical.fit(X, invoice_data.is_ebay, \n",
    "                  batch_size =batch_size,validation_data=(X_test, invoice_test[invoice_test['category'].isin(le.classes_)].is_ebay), \n",
    "                   epochs =2,  verbose = 1)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "syv1fstFVNGw",
    "outputId": "5fe71686-74a3-40be-9c2b-3a6dc9ab24bc"
   },
   "outputs": [],
   "source": [
    "# #building goods and services transffered models with invoice data\n",
    "le_service = LabelEncoder()\n",
    "le_goods = LabelEncoder()\n",
    "\n",
    "y_service = le_service.fit_transform(invoice_data[(~invoice_data.is_ebay.values)]['true_indy_name'].astype(str).values)\n",
    "y_goods = le_goods.fit_transform(invoice_data[(invoice_data.is_ebay.values)]['true_indy_name'].astype(str).values)\n",
    "\n",
    "test_informative = invoice_test[(invoice_test['category'].isin(le_service.classes_)) | (invoice_test['category'].isin(le_goods.classes_))]\n",
    "\n",
    "\n",
    "y_test_goods, y_test_service = le_goods.transform(invoice_test.category[invoice_test.category.isin(le_goods.classes_)]), le_service.transform(invoice_test.category[invoice_test.category.isin(le_service.classes_)])\n",
    "inp_goods = Input(shape=(maxlen, ))\n",
    "\n",
    "x_goods = Embedding(num_words, embed_dim,input_length = X.shape[1], trainable=False, weights=[embedding_matrix])(inp_goods)\n",
    "# x = Embedding(num_words, embed_dim,input_length = X.shape[1], trainable=True)(inp)\n",
    "x_goods = SpatialDropout1D(0.3)(x_goods)\n",
    "x_goods = LSTM(lstm_out, return_sequences=True)(x_goods)\n",
    "x_goods = LSTM(100)(x_goods)\n",
    "x_goods = Dropout(0.2)(x_goods)\n",
    "dens_goods = Dense(30)(x_goods)\n",
    "outp_goods = Dense(len(le_goods.classes_), activation=\"softmax\")(dens_goods)\n",
    "    \n",
    "model_goods = Model(inputs=inp_goods, outputs=outp_goods)\n",
    "model_goods.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "inp_service = Input(shape=(maxlen, ))\n",
    "\n",
    "x_service = Embedding(num_words, embed_dim,input_length = X.shape[1], trainable=False, weights=[embedding_matrix])(inp_service)\n",
    "# x = Embedding(num_words, embed_dim,input_length = X.shape[1], trainable=True)(inp)\n",
    "x_service = SpatialDropout1D(0.3)(x_service)\n",
    "x_service = LSTM(lstm_out, return_sequences=True)(x_service)\n",
    "x_service = LSTM(100)(x_service)\n",
    "x_service = Dropout(0.2)(x_service)\n",
    "dens_service = Dense(30)(x_service)\n",
    "outp_service = Dense(len(le_service.classes_), activation=\"sigmoid\")(dens_service)\n",
    "    \n",
    "model_service = Model(inputs=inp_service, outputs=outp_service)\n",
    "model_service.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "model_goods.fit(X[invoice_data.is_ebay.values],y_goods,batch_size =batch_size,validation_data=(X_test[test_informative.category.isin(le_goods.classes_)], y_test_goods), epochs = 5,  verbose = 1)\n",
    "\n",
    "model_service.fit(X[~invoice_data.is_ebay.values],y_service,batch_size =batch_size,validation_data=(X_test[test_informative.category.isin(le_service.classes_)], y_test_service), epochs =8,  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DPbixd0ggFy9"
   },
   "outputs": [],
   "source": [
    "#building ebay transffered model\n",
    "X_ebay = tokenizer.texts_to_sequences(ebay_data['text'][ebay_data['vertical'].isin(le_goods.classes_)].values)\n",
    "X_ebay = pad_sequences(X_ebay,maxlen=maxlen)\n",
    "Y_ebay =le_goods.transform(ebay_data['vertical'][ebay_data['vertical'].isin(le_goods.classes_)].astype(str)).reshape(-1, 1)\n",
    "\n",
    "inp_ebay = Input(shape=(maxlen, ))\n",
    "\n",
    "x_ebay = Embedding(num_words, embed_dim,input_length = X.shape[1], trainable=False, weights=[embedding_matrix])(inp_ebay)\n",
    "# x = Embedding(num_words, embed_dim,input_length = X.shape[1], trainable=True)(inp)\n",
    "x_ebay = SpatialDropout1D(0.3)(x_ebay)\n",
    "x_ebay = LSTM(lstm_out, return_sequences=True)(x_ebay)\n",
    "x_ebay = LSTM(100)(x_ebay)\n",
    "x_ebay = Dropout(0.2)(x_ebay)\n",
    "dens_ebay = Dense(30)(x_ebay)\n",
    "outp_ebay = Dense(len(le_goods.classes_), activation=\"sigmoid\")(dens_ebay)\n",
    "    \n",
    "model_ebay = Model(inputs=inp_ebay, outputs=outp_ebay)\n",
    "model_ebay.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "model_ebay.fit(X_ebay,Y_ebay,batch_size =batch_size,validation_data=(X_test[test_informative.category.isin(le_goods.classes_)], y_test_goods), epochs =7,  verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGPgRkPIBAHU"
   },
   "outputs": [],
   "source": [
    "#set embedding model\n",
    "emb_model_goods = Model(inputs=inp_goods, outputs=dens_goods)\n",
    "emb_model_service = Model(inputs=inp_service, outputs=dens_service)\n",
    "emb_model_ebay = Model(inputs=inp_ebay, outputs=dens_ebay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uD_hu6qvAffP"
   },
   "outputs": [],
   "source": [
    "#extract transffered embeddings on the manually labelled mturk data\n",
    "\n",
    "\n",
    "X_finetune = tokenizer.texts_to_sequences(invoice_train_manual['text'])\n",
    "X_finetune = pad_sequences(X_finetune,maxlen=maxlen)\n",
    "y_finetune = invoice_train_manual['category']\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(invoice_test['text'])\n",
    "X_test = pad_sequences(X_test,maxlen=maxlen)\n",
    "y_test = invoice_test['category']\n",
    "\n",
    "goods_emb = emb_model_goods.predict(X_finetune)\n",
    "service_emb = emb_model_service.predict(X_finetune)\n",
    "ebay_emb = emb_model_ebay.predict(X_finetune)\n",
    "\n",
    "test_goods_emb = emb_model_goods.predict(X_test)\n",
    "test_service_emb = emb_model_service.predict(X_test)\n",
    "test_ebay_emb = emb_model_ebay.predict(X_test)\n",
    "\n",
    "invoice_goods_emb = emb_model_goods.predict(X,batch_size=5000,verbose=1)\n",
    "invoice_service_emb = emb_model_service.predict(X,batch_size=5000,verbose=1)\n",
    "invoice_ebay_emb = emb_model_ebay.predict(X,batch_size=5000,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    \n",
    "res = [[],[],[],[],[],[],[],[],[],[]]\n",
    "res2 = [[],[],[],[],[],[],[],[],[],[]]\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits=10,shuffle=True)\n",
    "for train_index, test_index in kf.split(invoice_test, invoice_test.category):\n",
    "    print('------iter------')\n",
    "    X_finetune = tokenizer.texts_to_sequences(invoice_test['text'].values)\n",
    "    X_finetune = pad_sequences(X_finetune,maxlen=maxlen)\n",
    "    y_finetune = invoice_test['category']\n",
    "\n",
    "    goods_emb = emb_model_goods.predict(X_finetune)\n",
    "    service_emb = emb_model_service.predict(X_finetune)\n",
    "    ebay_emb = emb_model_ebay.predict(X_finetune)\n",
    "    auto_emb = encoder_model.predict(X_finetune)\n",
    "    invoice_emb = emb_model_invoice.predict(X_finetune)\n",
    "\n",
    "    emb_train = np.concatenate((goods_emb,service_emb,ebay_emb,auto_emb),axis=1)\n",
    "    emb_auto_invoice = np.concatenate((goods_emb,service_emb,auto_emb),axis=1)\n",
    "    emb_auto_ebay = np.concatenate((ebay_emb,auto_emb),axis=1)\n",
    "    emb_ebay_invoice = np.concatenate((goods_emb,service_emb,ebay_emb),axis=1)\n",
    "\n",
    "    ind_train_goods = (invoice_test.is_ebay.fillna(False).values) &  (invoice_test.index.isin(train_index))\n",
    "    ind_test_goods =  (invoice_test.is_ebay.fillna(False).values)  &  (invoice_test.index.isin(test_index))\n",
    "    ind_train_service = (~invoice_test.is_ebay.fillna(False).values)  &  (invoice_test.index.isin(train_index))\n",
    "    ind_test_service = (~invoice_test.is_ebay.fillna(False).values)  & (invoice_test.index.isin(test_index))\n",
    "    ind_train_all = (invoice_test.index.isin(train_index)) \n",
    "    ind_test_all = (invoice_test.index.isin(test_index)) \n",
    "\n",
    "\n",
    "    lr_goods = MLPClassifier()\n",
    "    lr_goods.fit(goods_emb[ind_train_all],y_finetune[ind_train_all])\n",
    "    print(lr_goods.score(goods_emb[ind_test_all],y_finetune[ind_test_all]))\n",
    "    lr_goods_pred = lr_goods.predict(goods_emb[ind_test_all])\n",
    "    print(f1_score(y_finetune[ind_test_all], lr_goods_pred, average='weighted'))\n",
    "    res[0].append(lr_goods.score(goods_emb[ind_test_all],y_finetune[ind_test_all]))\n",
    "    res2[0].append(f1_score(y_finetune[ind_test_all], lr_goods_pred, average='weighted'))\n",
    "\n",
    "    lr_service = MLPClassifier()\n",
    "    lr_service.fit(service_emb[ind_train_all],y_finetune[ind_train_all])\n",
    "    print(lr_service.score(service_emb[ind_test_all],y_finetune[ind_test_all]))\n",
    "    lr_service_pred = lr_service.predict(service_emb[ind_test_all])\n",
    "    print(f1_score(y_finetune[ind_test_all], lr_service_pred, average='weighted'))\n",
    "    res[1].append(lr_service.score(service_emb[ind_test_all],y_finetune[ind_test_all]))\n",
    "    res2[1].append(f1_score(y_finetune[ind_test_all], lr_service_pred, average='weighted'))\n",
    "\n",
    "    lr_ebay = MLPClassifier()\n",
    "    lr_ebay.fit(ebay_emb[ind_train_all],y_finetune[ind_train_all])\n",
    "    print(lr_ebay.score(ebay_emb[ind_test_all],y_finetune[ind_test_all]))\n",
    "    lr_ebay_pred = lr_ebay.predict(ebay_emb[ind_test_all])\n",
    "    print(f1_score(y_finetune[ind_test_all], lr_ebay_pred, average='weighted'))\n",
    "    res[2].append(lr_ebay.score(ebay_emb[ind_test_all],y_finetune[ind_test_all]))\n",
    "    res2[2].append(f1_score(y_finetune[ind_test_all], lr_ebay_pred, average='weighted'))\n",
    "\n",
    "    lr_auto = MLPClassifier()\n",
    "    lr_auto.fit(auto_emb[ind_train_all],y_finetune[ind_train_all])\n",
    "    print(lr_auto.score(auto_emb[ind_test_all],y_finetune[ind_test_all]))\n",
    "    lr_auto_pred = lr_auto.predict(auto_emb[ind_test_all])\n",
    "    print(f1_score(y_finetune[ind_test_all], lr_auto_pred, average='weighted'))\n",
    "    res[3].append(lr_auto.score(auto_emb[ind_test_all],y_finetune[ind_test_all]))\n",
    "    res2[3].append(f1_score(y_finetune[ind_test_all], lr_auto_pred, average='weighted'))\n",
    "\n",
    "    lr_invoice = MLPClassifier()\n",
    "    lr_invoice.fit(invoice_emb[ind_train_all],y_finetune[ind_train_all])\n",
    "    print(lr_invoice.score(invoice_emb[ind_test_all],y_finetune[ind_test_all]))\n",
    "    lr_invoice_pred = lr_invoice.predict(invoice_emb[ind_test_all])\n",
    "    print(f1_score(y_finetune[ind_test_all], lr_invoice_pred, average='weighted'))\n",
    "    res[4].append(lr_invoice.score(invoice_emb[ind_test_all],y_finetune[ind_test_all]))\n",
    "    res2[4].append(f1_score(y_finetune[ind_test_all], lr_invoice_pred, average='weighted'))\n",
    "\n",
    "    lr_emb = LogisticRegression()\n",
    "    lr_emb.fit(emb_train[ind_train_all],y_finetune.values[ind_train_all])\n",
    "    print(lr_emb.score(emb_train[ind_test_all],y_finetune.values[ind_test_all]))\n",
    "    lr_emb_pred = lr_emb.predict(emb_train[ind_test_all])\n",
    "    print(f1_score(y_finetune[ind_test_all], lr_emb_pred, average='weighted'))\n",
    "    res[5].append(lr_emb.score(emb_train[ind_test_all],y_finetune.values[ind_test_all]))\n",
    "    res2[5].append(f1_score(y_finetune[ind_test_all], lr_emb_pred, average='weighted'))\n",
    "\n",
    "\n",
    "    lr_meta = MLPClassifier()\n",
    "    # meta = np.concatenate([lr_goods.predict_proba(goods_emb),lr_service.predict_proba(service_emb),\n",
    "    #                       lr_ebay.predict_proba(ebay_emb),lr_auto.predict_proba(auto_emb)],axis=1)\n",
    "    meta = np.concatenate([lr_invoice.predict_proba(invoice_emb),\n",
    "                        lr_ebay.predict_proba(ebay_emb),lr_auto.predict_proba(auto_emb)],axis=1)\n",
    "    lr_meta.fit(meta[ind_train_all],y_finetune.values[ind_train_all])\n",
    "    print(lr_meta.score(meta[ind_test_all],y_finetune.values[ind_test_all]))\n",
    "    lr_meta_pred = lr_meta.predict(meta[ind_test_all])\n",
    "    print(f1_score(y_finetune[ind_test_all], lr_meta_pred, average='weighted'))\n",
    "    res[6].append(lr_meta.score(meta[ind_test_all],y_finetune.values[ind_test_all]))\n",
    "    res2[6].append(f1_score(y_finetune[ind_test_all], lr_meta_pred, average='weighted'))\n",
    "\n",
    "    lr_emb_ebay_invoice = LogisticRegression()\n",
    "    lr_emb_ebay_invoice.fit(emb_ebay_invoice[ind_train_all],y_finetune.values[ind_train_all])\n",
    "    print(lr_emb_ebay_invoice.score(emb_ebay_invoice[ind_test_all],y_finetune.values[ind_test_all]))\n",
    "    lr_emb_ebay_invoice_pred = lr_emb_ebay_invoice.predict(emb_ebay_invoice[ind_test_all])\n",
    "    print(f1_score(y_finetune[ind_test_all], lr_emb_ebay_invoice_pred, average='weighted'))\n",
    "    res[7].append(lr_emb_ebay_invoice.score(emb_ebay_invoice[ind_test_all],y_finetune.values[ind_test_all]))\n",
    "    res2[7].append(f1_score(y_finetune[ind_test_all], lr_emb_ebay_invoice_pred, average='weighted'))\n",
    "\n",
    "    lr_emb_auto_incoice = LogisticRegression()\n",
    "    lr_emb_auto_incoice.fit(emb_auto_invoice[ind_train_all],y_finetune.values[ind_train_all])\n",
    "    print(lr_emb_auto_incoice.score(emb_auto_invoice[ind_test_all],y_finetune.values[ind_test_all]))\n",
    "    lr_emb_auto_invoice_pred = lr_emb_auto_incoice.predict(emb_auto_invoice[ind_test_all])\n",
    "    print(f1_score(y_finetune[ind_test_all], lr_emb_auto_invoice_pred, average='weighted'))\n",
    "    res[8].append(lr_emb_auto_incoice.score(emb_auto_invoice[ind_test_all],y_finetune.values[ind_test_all]))\n",
    "    res2[8].append(f1_score(y_finetune[ind_test_all], lr_emb_auto_invoice_pred, average='weighted'))\n",
    "\n",
    "    lr_emb_auto_ebay = LogisticRegression()\n",
    "    lr_emb_auto_ebay.fit(emb_auto_ebay[ind_train_all],y_finetune.values[ind_train_all])\n",
    "    print(lr_emb_auto_ebay.score(emb_auto_ebay[ind_test_all],y_finetune.values[ind_test_all]))\n",
    "    lr_emb_auto_ebay_pred = lr_emb_auto_ebay.predict(emb_auto_ebay[ind_test_all])\n",
    "    print(f1_score(y_finetune[ind_test_all], lr_emb_auto_ebay_pred, average='weighted'))\n",
    "    res[9].append(lr_emb_auto_ebay.score(emb_auto_ebay[ind_test_all],y_finetune.values[ind_test_all]))\n",
    "    res2[9].append(f1_score(y_finetune[ind_test_all], lr_emb_auto_ebay_pred, average='weighted'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Ensemblled transferred embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
